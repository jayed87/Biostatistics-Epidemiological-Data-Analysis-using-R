---
title: "Solutions to additional homework 9"
author: "Stefan Konigorski"
date: "December 14, 2022"
output:
  html_document: default
---

## Load KiGGS data

```{r}
dat_link <- url("https://www.dropbox.com/s/pd0z829pv2otzqt/KiGGS03_06.RData?dl=1")
load(dat_link)
dat <- KiGGS03_06
```

## Examine assumptions of exercise 1

**Task:** 

Load the KiGGS dataset and compute a regression predicting BMI by sex and age groups (age2). In this model, investigate and judge whether the assumptions listed on slide 13 in lecture 9 are satisfied.

**Solution:** 

```{r}
# variable preparation
bmi <- dat$bmiB
sex <- dat$sex
age <- dat$age2

# regression
fit1 <- lm(bmi ~ sex + as.numeric(age))

# results:
summary(fit1)$coefficients

# compute same model with log-transformed BMI outcome (this is done after inspecting the assumption that the residuals are normally distributed below, which showed that they are skewed):
bmi_log <- log(bmi)
fit2 <- lm(bmi_log ~ sex + age)
summary(fit2)$coefficients
```

Look at assumptions:

(1) If Y continuous? I.e. is BMI a continuous variable? -> both theoretically and in the: yes

(2) relationship between the X variables and Y linear?

Look at scatterplots:

```{r}
plot(as.numeric(sex), bmi)
plot(as.numeric(age), bmi)

# a slightly different way of looking at it, adding jitter:
plot(jitter(as.numeric(age)), bmi)
```

-> These plots might suggest that there can be a U-shape relationship (minimum at 2-3) and not a linear relationship, but it is hard to say from these plots.

Look at boxplots with added regression line (you have to run both lines in the code together):

```{r}
plot(sex, bmi)
abline(lm(bmi ~ as.numeric(sex)), col = "blue")

plot(age, bmi)
abline(lm(bmi ~ as.numeric(age)), col = "blue")
```

This is already a lot more informative, both looking at boxplots, and also adding the regression line (with the respective predictor) for illustration (just for illustration, since it is not the same regression as our full model with both variables). This suggests that assuming a linear relationship is not optimal for age.

Other way to look at it: look at residual plot.

```{r}
plot(fit1, 1)
```

This give some information, but is less clear visible and NOT the best way to check for linearity. It can give the following information: for low (fitted) BMI values the errors are rather positive and for the medium (fitted) BMI values the errors are rather negative, which supports that the regression line is not optimal. However, these deviances are only small and this looks at the regression line for both variables, hence it only gives indirect information that the regression line is not optimal so that you should check e.g. for linearity by looking at above plots.

One way how to incorporate non-linearity:

```{r}
# regression models with quadratic terms:
fit_quadr <- lm(bmi ~ as.numeric(age) + I(as.numeric(age)^2))
summary(fit_quadr)$coefficients
```

These clearly confirm that the relationship is not linear. Visualize this model:

```{r}
fit_quadr_function <- function(x){summary(fit_quadr)$coefficients[1,1] +   
                                   summary(fit_quadr)$coefficients[2,1]*x + 
                                   summary(fit_quadr)$coefficients[3,1]*x^2}
plot(age, bmi)
plot(fit_quadr_function, xlim = c(1, 9), add = TRUE, col = "blue")
```

This looks better. 

Another way of investigating and potentially further improving it is by checking whether the regression coefficients of as.numeric(age) are similar to the differences between categories when age is used in a regression model as factor. 

```{r}
summary(lm(bmi ~ as.numeric(age)))$coefficients
summary(lm(bmi ~ age))$coefficients
```

-> This also suggests that the relationship is not linear. Hence age should be modeled as factor not with as.numeric, or with a quadratic term. 

(3) All relevant variables (covariates, confounders) are in the model

-> cannot be tested, this would be our assumption. However, here, this assumption does not seem to be correct: I can think of many other variables that can potentially have an effect.

(4) All observations are independent

-> there might be some structure in that students are clustered in schools or districts, or are siblings, but we don't have that information and there is a large sample size, so that assuming they are independent might be ok.

Can also e.g. compute and test the ICC (intraclass correlation coefficient) using the ICC() function, or test for autocorrelation using the lmtest::dwtest() function to check this.

(5) No multicollinearity

-> fit1 looks totally fine, and there is no correlation between predictors:

```{r}
cor(as.numeric(age), as.numeric(sex), use = "complete.obs", method = "spearman")
```

Can also check this e.g. by using the car::vif() function.

(6) Homoscedastic residuals

```{r}
# Look at first plot in:
plot(fit1)
```

-> higher variance for fitted values, but there are also more observations there, so maybe this is ok.

Can also check this e.g. by using the ncvTest() function.

(7) Normally-distributed residuals

```{r}
# Distribution of outcome:
hist(bmi)
qqnorm(scale(bmi)); abline(0,1)
hist(bmi_log)
qqnorm(scale(bmi_log)); abline(0,1)

# Distribution of residuals, see:
plot(fit1, 2)
plot(fit2, 2)
```

-> residuals in log-transformed BMI model look slightly better, but still not normally-distributed.

CONCLUSION: there is quite some evidence that several assumptions are violated in this regression model, so that the model should be improved!

Addition 1: The gvlma function can be used to perform hypothesis tests of several assumptions (with the caveat that a visual inspection of some assumptions is preferred, see our discussions in the tutorial): 

```{r}
# use glmva package
#install.packages("gvlma")
library(gvlma)
gvmodel <- gvlma(fit1)
summary(gvmodel)
```

Addition 2: Further plots for diagnostics of the regression model are available, e.g. using:

```{r}
plot(fit1, 4)
plot(fit1, 5)
plot(fit1, 6)
```
Here, Cook's distance is a measure of the influence of each data point on the estimated regression line. Leverage looks at something similar: how far away is a data point from the others and therewith influencing the estimated regression line. Both metrics can point to influential points e.g. outliers that can be checked.

## Exercise 2: Model selection in linear regression (optional)

**Task:** 

In the KiGGS dataset, aim to select relevant predictors for sys12 (systolic blood pressure). Use 2 of the model selection approaches described on slide 26, apply them to the KiGGS dataset and compare the results.

**Solution:** 

Lets compare lasso and stepwise model selection using the same predictors.

Stepwise variable selection:

```{r}
# format variables and get complete case dataset:
dat_select <- data.frame(sys = as.numeric(as.character(dat$sys12)), age = as.numeric(dat$age2), sex = dat$sex, bmi = as.numeric(as.character(dat$bmiB)), chol = as.numeric(as.character(dat$CHOLX)), gluc = as.numeric(as.character(dat$GLUCX)), iron = as.numeric(as.character(dat$Eisen)))
dat_select <- dat_select[complete.cases(dat_select), ]
dim(dat_select)
# -> ok good, still 13034 observations

library(MASS)

nullmodel <- lm(sys ~ 1, dat = dat_select)
fullmodel <- lm(sys ~ age + sex + bmi + chol + gluc + iron, dat = dat_select)

stepAIC(object = nullmodel, scope = list(upper = fullmodel, lower = nullmodel), direction = "forward")$anova
stepAIC(object = fullmodel, scope = list(upper = fullmodel, lower = nullmodel), direction = "backward")$anova
```

Lasso:

```{r}
library(glmnet)

# preparation: extract all predictors and transform from data.frame to matrix:
predictors <- dat_select[, 2:7]
predictors$sex <- as.numeric(predictors$sex)
predictors <- as.matrix(predictors)

# lasso:
fit_lasso <- cv.glmnet(x = predictors, y = dat_select$bmi, alpha = 1, family = "gaussian")
coef(fit_lasso, s = "lambda.min")
```

-> while both stepwise selection models select age, sex, bmi, glucose and iron into the final model as relevant variables, lasso only selects bmi!

## Exercise 3: Linear regression with multiple imputation (optional)

**Task:** 

Run the code in the Rmd file R_9b_linear_regression_MI.Rmd, inspect the R code what it is doing, and look at the results. Apply the same to the linear regression model of another variable of your choice.

**Solution:** 

```{r}
library(mice)

# step 1: do imputation, generate 5 imputed datasets, use method 'norm' instead of 'pmm' here since it is faster
tempData <- mice(dat_select, m = 5, maxit = 50, meth = 'norm', seed = 500)

# step 2: do the statistical analysis with help of the "with" function
fit_mi <- with(tempData, lm(sys ~ age + sex + bmi + chol + gluc + iron))

# step 3: pool the results
summary(pool(fit_mi))
```

